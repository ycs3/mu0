{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MuZero\n",
    "\n",
    "Based on MuZero pseudocode:\n",
    "\n",
    " * https://arxiv.org/src/1911.08265v2/anc/pseudocode.py\n",
    " \n",
    "TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gXtuMtMvlQoI"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrT8jxwQlQoT"
   },
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self): # no recovery state, no discount\n",
    "        self.action_space_size = 9\n",
    "        self.raw_board = [0 for i in range(self.action_space_size)]\n",
    "        self.to_play = 1 # alternates between 0 and 1\n",
    "        self.terminated = False\n",
    "        self.raw_reward = 0\n",
    "        \n",
    "        self.initial_board = self.board(perspective=\"to_play\")\n",
    "        \n",
    "        # actual board history \"before playing\" would be:\n",
    "        #  game.make_image(-1) + game.board_history\n",
    "        self.board_history = [] # board state after playing (perspective of next player)\n",
    "        # rewards will need to be back-filled\n",
    "        self.rewards = [] # reward after playing (perspective of player)\n",
    "        self.history = [] # action (that player) executed\n",
    "        self.player_history = [] # player that executed action\n",
    "        \n",
    "        self.child_visits = []\n",
    "        self.root_values = []\n",
    "        \n",
    "    def backfill_rewards(self):\n",
    "        # once game is terminated (last player plays - self.played)\n",
    "        # back-fill rewards based on player_history perspective\n",
    "        self.rewards = [self.reward(perspective=p) for p in self.player_history]\n",
    "        \n",
    "    def terminal(self):\n",
    "        return self.terminated\n",
    "    \n",
    "    def legal_actions(self):\n",
    "        ret = []\n",
    "        if self.terminated:\n",
    "            return ret\n",
    "        # non-zero locations can be valid actions\n",
    "        for idx, val in enumerate(self.raw_board):\n",
    "            if val == 0:\n",
    "                ret.append(idx)\n",
    "        return ret\n",
    "        \n",
    "    def apply(self, action):\n",
    "        self.player_history.append(self.to_play)\n",
    "        self.history.append(action)\n",
    "        reward = self.step(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.board_history.append(self.board(perspective=\"to_play\"))\n",
    "        return self\n",
    "    \n",
    "    def store_search_statistics(self, root):\n",
    "        sum_visits = sum(child.visit_count for child in root.children.values())\n",
    "        if sum_visits == 0:\n",
    "            sum_visits = 1\n",
    "        action_space = [index for index in range(self.action_space_size)]\n",
    "        self.child_visits.append([\n",
    "            root.children[a].visit_count / sum_visits if a in root.children else 0\\\n",
    "            for a in action_space\n",
    "        ])\n",
    "        self.root_values.append(root.value())\n",
    "        \n",
    "    def make_image(self, state_index=None):\n",
    "        # state_index starts at 0, after first play\n",
    "        # image is after the play, in the perspective of next_player\n",
    "        # to get initial state, get self.initial_board (state_index = -1)\n",
    "        # no state_index specifies the most recent board\n",
    "        # batch of 1\n",
    "        if state_index == -1:\n",
    "            return [self.initial_board]\n",
    "        if state_index is None:\n",
    "            if len(self.board_history) == 0:\n",
    "                return [self.initial_board]\n",
    "            state_index = -1\n",
    "\n",
    "        return [self.board_history[state_index]]\n",
    "        \n",
    "    def make_target(self, state_index, num_unroll_steps):\n",
    "        # the value target is the discounted root value of the search tree N steps\n",
    "        # into the future, plus the discounted sum of all rewards until then\n",
    "        # (there is no discount for this game, end reward is applied to all steps)\n",
    "        # (there is no bootstrap index, this will always be the end of the game)\n",
    "        # (value is the reward in our case?)\n",
    "        targets = []\n",
    "        for current_index in range(state_index, state_index+num_unroll_steps+1):\n",
    "            if current_index < len(self.root_values):\n",
    "                targets.append((\n",
    "                    self.rewards[current_index], self.rewards[current_index],\n",
    "                    self.child_visits[current_index]\n",
    "                ))\n",
    "            #else:\n",
    "            #    targets.append((0, 0, []))\n",
    "        return targets\n",
    "\n",
    "    def board(self, perspective=None):\n",
    "        # perspective\n",
    "        # - None: raw_board\n",
    "        # - played: \"played\" player's perspective\n",
    "        # - to_play: \"to_play\" player's perspective\n",
    "        # - -1, 1: selected player's perspective\n",
    "        if perspective is None:\n",
    "            return self.raw_board\n",
    "        elif perspective == \"played\":\n",
    "            return [k * -self.to_play for k in self.raw_board]\n",
    "        elif perspective == \"to_play\":\n",
    "            return [k * self.to_play for k in self.raw_board]\n",
    "        return [k * perspective for k in self.raw_board]\n",
    "    \n",
    "    def _eval_win(self):\n",
    "        # check if player has won - enumerate all win positions\n",
    "        # self.to_play is the player that just played\n",
    "        win_positions = np.array([\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "            [0, 4, 8], [2, 4, 6],\n",
    "        ])\n",
    "        has_win = np.any(\n",
    "            np.all((\n",
    "                np.array(self.raw_board)[win_positions.ravel()] == self.to_play\n",
    "            ).reshape(*win_positions.shape), axis=1)\n",
    "        )\n",
    "        return has_win\n",
    "    \n",
    "    def step(self, action, debug=False):\n",
    "        # manual environment step\n",
    "        if self.raw_board[action] == 0:\n",
    "            self.raw_board[action] = self.to_play\n",
    "        else:\n",
    "            # invalid action\n",
    "            if debug:\n",
    "                print(\"step: invalid action\")\n",
    "            if self.terminated is False:\n",
    "                # automatic win for other player\n",
    "                self.raw_reward = -self.to_play\n",
    "            self.terminated = True\n",
    "\n",
    "        # self.to_play player just played\n",
    "        if self._eval_win():\n",
    "            if debug:\n",
    "                print(\"step: eval_win\")\n",
    "            if self.terminated is False:\n",
    "                self.raw_reward = self.to_play\n",
    "            self.terminated = True\n",
    "\n",
    "        if len(self.legal_actions()) == 0:\n",
    "            if debug:\n",
    "                print(\"step: no more valid actions\")\n",
    "            # tie if not already terminated?\n",
    "            self.terminated = True\n",
    "            \n",
    "        # player played, reverse roles\n",
    "        self.to_play = -self.to_play\n",
    "\n",
    "        return self.reward(perspective=\"played\")\n",
    "    \n",
    "    def reward(self, perspective=None):\n",
    "        if perspective is None:\n",
    "            return self.raw_reward\n",
    "        elif perspective == \"played\":\n",
    "            return self.raw_reward * -self.to_play\n",
    "        elif perspective == \"to_play\":\n",
    "            return self.raw_reward * self.to_play\n",
    "        return self.raw_reward * perspective\n",
    "    \n",
    "    def render(self):\n",
    "        for j in range(3):\n",
    "            print(\"[\", end=\" \")\n",
    "            for i in range(3):\n",
    "                val = self.raw_board[j * 3 + i]\n",
    "                s = \" \"\n",
    "                if val == 1:\n",
    "                    s = \"O\"\n",
    "                elif val == -1:\n",
    "                    s = \"X\"\n",
    "                print(s, end=\" \")\n",
    "            print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04qgybsYlQoa"
   },
   "outputs": [],
   "source": [
    "class Network:\n",
    "    def __init__(self, game_class, state_size=32, hidden_size=64):\n",
    "        self.observation_size = len(game_class().board())\n",
    "        self.action_size = game_class().action_space_size\n",
    "        self.state_size = state_size\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.representation_nn = nn.Sequential(\n",
    "            nn.Linear(self.observation_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.state_size)\n",
    "        )\n",
    "\n",
    "        self.dynamics_nn = nn.Sequential(\n",
    "            nn.Linear(self.state_size + self.action_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.state_size)\n",
    "        )\n",
    "        #self.reward_nn = nn.Sequential(\n",
    "        #    nn.Linear(self.state_size, self.hidden_size),\n",
    "        #    nn.ReLU(),\n",
    "        #    nn.Linear(self.hidden_size, 1),\n",
    "        #    nn.Tanh()\n",
    "        #)\n",
    "\n",
    "        self.value_nn = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, 1),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.policy_nn = nn.Sequential(\n",
    "            nn.Linear(self.state_size, self.hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_size, self.action_size),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "    def initial_inference(self, image):\n",
    "        image_t = torch.tensor(image, dtype=torch.float32)\n",
    "        state_t = self.representation_nn(image_t)\n",
    "        value_t = self.value_nn(state_t)\n",
    "        policy_t = self.policy_nn(state_t)\n",
    "        return [value_t, None, policy_t, state_t]\n",
    "    \n",
    "    def recurrent_inference(self, hidden_state, action_onehot):\n",
    "        #hidden_state_t = torch.tensor(hidden_state, dtype=torch.float32)\n",
    "        hidden_state_t = hidden_state\n",
    "        action_t = torch.tensor(action_onehot, dtype=torch.float32)\n",
    "        input_t = torch.cat([hidden_state_t, action_t], dim=1)\n",
    "        next_state_t = self.dynamics_nn(input_t)\n",
    "        #reward_t = self.reward_nn(next_state_t)\n",
    "        reward_t = None\n",
    "        value_t = self.value_nn(next_state_t)\n",
    "        policy_t = self.policy_nn(next_state_t)\n",
    "        return [value_t, reward_t, policy_t, next_state_t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Kg2vMZelQod"
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, prior, node_id=None):\n",
    "        if type(prior) == torch.Tensor:\n",
    "            prior = prior.item()\n",
    "        \n",
    "        self.node_id = node_id # debugging purposes\n",
    "        \n",
    "        self.visit_count = 0\n",
    "        self.to_play = -1\n",
    "        self.prior = prior\n",
    "        self.value_sum = 0\n",
    "        self.children = {}\n",
    "        self.hidden_state = None\n",
    "        self.reward = 0\n",
    "\n",
    "    def expanded(self):\n",
    "        return len(self.children) > 0\n",
    "\n",
    "    def value(self):\n",
    "        if self.visit_count == 0:\n",
    "            return 0\n",
    "        return self.value_sum / self.visit_count\n",
    "    \n",
    "    def __str__(self):\n",
    "        return \"%s: prior: %5.2f, n_child: %d, n_visit: %d, value_sum: %d\" %\\\n",
    "            (str(self.node_id), self.prior, len(self.children), self.visit_count, self.value_sum)\n",
    "\n",
    "    def print_tree(self, depth=999):\n",
    "        # currently max depth = 4\n",
    "        print(self)\n",
    "        for i1 in self.children.keys():\n",
    "            n1 = self.children[i1]\n",
    "            print(\" \", n1)\n",
    "            if depth >= 2:\n",
    "                for i2 in n1.children.keys():\n",
    "                    n2 = n1.children[i2]\n",
    "                    print(\"   \", n2)\n",
    "                    if depth >= 3:\n",
    "                        for i3 in n2.children.keys():\n",
    "                            n3 = n2.children[i3]\n",
    "                            print(\"     \", n3)\n",
    "                            if depth >= 4:\n",
    "                                for i4 in n3.children.keys():\n",
    "                                    n4 = n3.children[i4]\n",
    "                                    print(\"       \", n4)\n",
    "                        \n",
    "def add_exploration_noise(node):\n",
    "    root_dirichlet_alpha = 0.5  # for chess, 0.03 for Go and 0.15 for shogi\n",
    "    root_exploration_fraction = .25\n",
    "    actions = node.children.keys()\n",
    "    noise = np.random.gamma(root_dirichlet_alpha, 1, len(actions))\n",
    "    frac = root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "def expand_node(node, to_play, actions, network_output):\n",
    "    #print(network_output)\n",
    "    node.to_play = to_play\n",
    "    node.hidden_state = network_output[3]\n",
    "    # ignore reward\n",
    "    policy = {a: network_output[2][0][a] for a in actions}\n",
    "    policy_sum = sum(policy.values())\n",
    "    for action, p in policy.items():\n",
    "        node.children[action] = Node(p / policy_sum, node.node_id + \"-\" + str(action))\n",
    "\n",
    "def ucb1(parent, child):\n",
    "    if False:\n",
    "        if child.visit_count == 0:\n",
    "            return np.inf\n",
    "        return (-child.value_sum / child.visit_count) + 2 * np.sqrt(np.log(parent.visit_count) / child.visit_count)\n",
    "    \n",
    "    prior_score = child.prior * np.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "    \n",
    "    return value_score + prior_score\n",
    "\n",
    "def backpropagate(search_path, value, to_play):\n",
    "    if type(value) == torch.Tensor:\n",
    "        value = value.item()\n",
    "    for node in search_path:\n",
    "        node.value_sum += value if node.to_play == to_play else -value\n",
    "        node.visit_count += 1\n",
    "        #print(node)\n",
    "\n",
    "def run_mcts(root, action_history, action_space, network, num_simulations=5):\n",
    "    for sim_idx in range(num_simulations):\n",
    "        #print(\"simulation:\", sim_idx)\n",
    "        history = action_history.copy()\n",
    "        node = root\n",
    "        search_path = [node]\n",
    "        \n",
    "        while node.expanded():\n",
    "            #print(node.children)\n",
    "            #for action, child in node.children.items():\n",
    "            #    print(\"+\", ucb1(node, child))\n",
    "            v, action, child = max(\n",
    "                (ucb1(node, child), action, child)\\\n",
    "                for action, child in node.children.items()\n",
    "            )\n",
    "            history.append(action)\n",
    "            #print(\"..\", action, v)\n",
    "            node = child\n",
    "            search_path.append(node)\n",
    "        \n",
    "        #print(search_path)\n",
    "        parent = search_path[-2]\n",
    "        action_onehot = [0 for _ in action_space]\n",
    "        action_onehot[history[-1]] = 1\n",
    "        network_output = network.recurrent_inference(\n",
    "            parent.hidden_state, [action_onehot]\n",
    "        )\n",
    "        to_play = (len(history) + 1) % 2 # specific to 2 player games\n",
    "        expand_node(node, to_play, action_space, network_output)\n",
    "        backpropagate(search_path, network_output[0], to_play)\n",
    "        #print(history, search_path)\n",
    "        #root.print_tree(depth=1)\n",
    "        #print(\">\", parent, search_path[-1])\n",
    "        \n",
    "def get_visit_counts(node, argmax=True):\n",
    "    db = []\n",
    "    g = node.restore_game()\n",
    "    visit_counts = [0 for i in range(len(g.raw_board))]\n",
    "    for n in node.children:\n",
    "        db.append([n, node.children[n].visit_count])\n",
    "        visit_counts[n] = node.children[n].visit_count\n",
    "    if len(db) == 0:\n",
    "        return None, visit_counts\n",
    "    #next_action = sorted(db, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    if argmax is True:\n",
    "        next_action = np.argmax(visit_counts)\n",
    "    else:\n",
    "        next_action = np.random.choice(len(visit_counts), p=np.array(visit_counts) / np.sum(visit_counts))\n",
    "    return next_action, visit_counts\n",
    "\n",
    "def select_action(num_moves, node, network, argmax=False):\n",
    "    visit_counts = [\n",
    "        (child.visit_count, action) for action, child in node.children.items()\n",
    "    ]\n",
    "    count_values = np.array(list(zip(*visit_counts))[0])\n",
    "    if np.sum(count_values) == 0:\n",
    "        count_values = count_values + 1\n",
    "    if argmax is True: # argmax\n",
    "        #print(visit_counts)\n",
    "        idx_sel = max([(k, j) for j, k in list(enumerate(visit_counts))])[1]\n",
    "    else:\n",
    "        idx_sel = np.random.choice(len(visit_counts), p=count_values/np.sum(count_values))\n",
    "    _, action = visit_counts[idx_sel]\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H0bmn8GWlQof"
   },
   "outputs": [],
   "source": [
    "def play_game(game_class, network, explore_threshold=4):\n",
    "    game = game_class()\n",
    "    action_space = list(range(game.action_space_size))\n",
    "    nodes = []\n",
    "    \n",
    "    while not game.terminal():\n",
    "        # At the root of the search tree we use the representation function to\n",
    "        # obtain a hidden state given the current observation.\n",
    "        root = Node(0, \"root\")\n",
    "        current_observation = game.make_image()\n",
    "        expand_node(root, game.to_play, game.legal_actions(),\n",
    "                    network.initial_inference(current_observation))\n",
    "        add_exploration_noise(root)\n",
    "\n",
    "        run_mcts(root, game.history, action_space, network, 100)\n",
    "        if len(game.history) < explore_threshold:\n",
    "            action = select_action(len(game.history), root, network, argmax=False)\n",
    "        else:\n",
    "            action = select_action(len(game.history), root, network, argmax=True)\n",
    "        #print(action)\n",
    "        #root.print_tree(depth=1)\n",
    "        game.apply(action)\n",
    "        game.store_search_statistics(root)\n",
    "        nodes.append(root)\n",
    "    game.backfill_rewards()\n",
    "    return game, nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yk8BN62QlQog"
   },
   "outputs": [],
   "source": [
    "game_class = TicTacToe\n",
    "network = Network(game_class)\n",
    "game, nodes = play_game(game_class, network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kjSRjLSKlQoh",
    "outputId": "40739b77-f099-41ca-d84b-12f8a6623163"
   },
   "outputs": [],
   "source": [
    "game.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e4I2nbBllQok",
    "outputId": "58c8e71d-02c1-47b6-99fa-17351011a2c0"
   },
   "outputs": [],
   "source": [
    "game.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7STQCExNlQop",
    "outputId": "2029f9a7-08ac-4d42-f786-3ed3046665d0"
   },
   "outputs": [],
   "source": [
    "nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8RWtdJmnlQoq",
    "outputId": "8407edf9-a7ba-4aa7-9845-96cdba0afe72"
   },
   "outputs": [],
   "source": [
    "nodes[0].print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n5wd2JyelQor"
   },
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, window_size=256):\n",
    "        self.window_size = window_size\n",
    "        self.buffer = []\n",
    "        \n",
    "    def save_game(self, game):\n",
    "        if len(self.buffer) > self.window_size:\n",
    "            self.buffer.pop(0)\n",
    "        self.buffer.append(game)\n",
    "    \n",
    "    def sample_batch(self, num_unroll_steps, batch_size=512):\n",
    "        games = [self.sample_game() for _ in range(batch_size)]\n",
    "        game_pos = [(g, self.sample_position(g)) for g in games]\n",
    "        # make_image -1 gives initial board, 0 gives after first action\n",
    "        return [(\n",
    "            g.make_image(i-1),\n",
    "            g.history[i:i+num_unroll_steps],\n",
    "            g.make_target(i, num_unroll_steps)\n",
    "        ) for (g, i) in game_pos]\n",
    "        \n",
    "    def sample_game(self):\n",
    "        game_idx = np.random.choice(len(self.buffer))\n",
    "        return self.buffer[game_idx]\n",
    "    \n",
    "    def sample_position(self, game):\n",
    "        position = np.random.choice(len(game.history))\n",
    "        return position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJ-VNMPWlQor"
   },
   "outputs": [],
   "source": [
    "def train_network(network, replay_buffer):\n",
    "    for i in range(50):\n",
    "        num_unroll_steps = 5\n",
    "        batch = replay_buffer.sample_batch(num_unroll_steps)\n",
    "        update_weights(optimizer, network, batch)\n",
    "    return network\n",
    "\n",
    "def update_weights(optimizer, network, batch):\n",
    "    for image, actions, targets in batch:\n",
    "        # reward is invalid\n",
    "        value, reward, policy_output, hidden_state = network.initial_inference(image)\n",
    "        predictions = [(1., value, reward, policy_output)]\n",
    "        \n",
    "        for action in actions:\n",
    "            value, reward, policy_output, hidden_state = network.recurrent_inference(hidden_state, action)\n",
    "            predictions.append((1./len(actions), value, reward, policy_output))\n",
    "        \n",
    "        for prediction, target in zip(predictions, targets):\n",
    "            gradient_scale, value, reward, policy_output = prediction\n",
    "            target_value, target_reward, target_policy = target\n",
    "            \n",
    "            l = (\n",
    "                scalar_loss(value, target_value) +\\\n",
    "                scalar_loss(reward, target_reward) +\\\n",
    "                tf.nn.softmax_cross_entropy_with_logits(\n",
    "                    logits=policy_logits, labels=target_policy\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KD69Q2DIlQos"
   },
   "outputs": [],
   "source": [
    "game_class = TicTacToe\n",
    "network = Network(game_class)\n",
    "replay_buffer = ReplayBuffer(192)\n",
    "\n",
    "optimizer = optim.Adam(\n",
    "    list(network.representation_nn.parameters()) +\\\n",
    "    list(network.dynamics_nn.parameters()) +\\\n",
    "    list(network.policy_nn.parameters()) +\\\n",
    "    list(network.value_nn.parameters()),\n",
    "    lr=.005\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sRQePQ5DvxXV",
    "outputId": "c3f55da9-efe1-4df7-89bb-ad3e88ee544c"
   },
   "outputs": [],
   "source": [
    "value_losses = []\n",
    "policy_losses = []\n",
    "for j in range(1200):\n",
    "    print(j, end=\" \")\n",
    "    for i in range(128):\n",
    "        print(\".\", end=\"\")\n",
    "        game, nodes = play_game(game_class, network)\n",
    "        replay_buffer.save_game(game)\n",
    "    print()\n",
    "\n",
    "    batch = replay_buffer.sample_batch(5, 256)\n",
    "\n",
    "    target_value_batch = []\n",
    "    value_batch = []\n",
    "    target_policy_batch = []\n",
    "    policy_batch = []\n",
    "\n",
    "    for image, actions, targets in batch:\n",
    "        value, reward, policy_output, hidden_state = network.initial_inference(image)\n",
    "        predictions = [(1., value, reward, policy_output)]\n",
    "\n",
    "        for action in actions:\n",
    "            action_onehot = [0 for i in range(9)]\n",
    "            action_onehot[action] = 1\n",
    "            value, reward, policy_output, hidden_state = network.recurrent_inference(hidden_state, [action_onehot])\n",
    "            predictions.append((1./len(actions), value, reward, policy_output))\n",
    "\n",
    "\n",
    "        for prediction, target in zip(predictions, targets):\n",
    "            gradient_scale, value, reward, policy_output = prediction\n",
    "            target_value, target_reward, target_policy = target\n",
    "            target_value_ = torch.tensor([target_value], dtype=torch.float32)\n",
    "            target_policy_ = torch.tensor(target_policy, dtype=torch.float32)\n",
    "\n",
    "            target_value_batch.append(target_value_)\n",
    "            value_batch.append(value[0])\n",
    "            target_policy_batch.append(target_policy_)\n",
    "            policy_batch.append(policy_output[0])\n",
    "\n",
    "    target_value_batch = torch.stack(target_value_batch)\n",
    "    value_batch = torch.stack(value_batch)\n",
    "    target_policy_batch = torch.stack(target_policy_batch)\n",
    "    policy_batch = torch.stack(policy_batch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    loss_value = F.mse_loss(\n",
    "        value_batch, target_value_batch\n",
    "    )\n",
    "    loss_policy = torch.mean(-torch.sum(\n",
    "        target_policy_batch *\\\n",
    "        torch.log(policy_batch),\n",
    "    dim=1))\n",
    "    print(loss_value, loss_policy)\n",
    "    value_losses.append(loss_value.item())\n",
    "    policy_losses.append(loss_policy.item())\n",
    "    loss = loss_value + loss_policy\n",
    "\n",
    "    loss.backward()\n",
    "    #print(loss)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(value_losses)\n",
    "plt.plot(policy_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sequence = []\n",
    "game = game_class()\n",
    "action_space = list(range(game.action_space_size))\n",
    "while True:\n",
    "    game.render()\n",
    "    print(\"avail:\", game.legal_actions())\n",
    "    if game.terminal():\n",
    "        print(\"reward:\", game.reward())\n",
    "        break\n",
    "        \n",
    "    print()\n",
    "    action = input()\n",
    "    action = int(action)\n",
    "    action_sequence.append(action)\n",
    "    game.apply(action)\n",
    "    \n",
    "    game.render()\n",
    "    print(\"avail:\", game.legal_actions())\n",
    "    if game.terminal():\n",
    "        print(\"reward:\", game.reward())\n",
    "        break\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    root = Node(0, \"root\")\n",
    "    current_observation = game.make_image()\n",
    "    expand_node(root, game.to_play, game.legal_actions(),\n",
    "                network.initial_inference(current_observation))\n",
    "    run_mcts(root, game.history, action_space, network, 100)\n",
    "    action = select_action(len(game.history), root, network, argmax=True)\n",
    "\n",
    "    action_sequence.append(action)\n",
    "    print(\"comp:\", action)\n",
    "    game.apply(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Mu0_SimpleGames_v1-rev2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
