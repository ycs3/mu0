{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaZero\n",
    "\n",
    "MCTS + Policy/Value Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def __init__(self, board=None, played=None, terminated=None, reward=None):\n",
    "        # board and reward is not \"perspective corrected\"\n",
    "        # player - O: 1, X: -1\n",
    "\n",
    "        # self.board_:\n",
    "        # [ 0 1 2\n",
    "        #   3 4 5\n",
    "        #   6 7 8 ]\n",
    "        if board is None:\n",
    "            self.raw_board = [0 for i in range(9)]\n",
    "        else:\n",
    "            self.raw_board = list(board)\n",
    "\n",
    "        # game starts with a change of player, so set to opponent\n",
    "        if played is None:\n",
    "            self.played = -1\n",
    "        else:\n",
    "            self.played = played\n",
    "\n",
    "        self.terminated = False if terminated is None else terminated\n",
    "\n",
    "        if reward is None:\n",
    "            self.raw_reward = 0\n",
    "        else:\n",
    "            self.raw_reward = reward\n",
    "\n",
    "    def state(self):\n",
    "        return {\n",
    "            \"board\": self.raw_board,\n",
    "            \"played\": self.played,\n",
    "            \"terminated\": self.terminated,\n",
    "            \"reward\": self.raw_reward\n",
    "        }\n",
    "\n",
    "    def valid_actions(self):\n",
    "        # non-zero locations can be valid actions\n",
    "        ret = []\n",
    "        if self.terminated:\n",
    "            return ret\n",
    "        for idx, val in enumerate(self.raw_board):\n",
    "            if val == 0:\n",
    "                ret.append(idx)\n",
    "        return ret\n",
    "\n",
    "    def board(self, perspective=None):\n",
    "        # perspective\n",
    "        # - None: raw_board\n",
    "        # - played: \"played\" player's perspective\n",
    "        # - to_play: \"to_play\" player's perspective\n",
    "        # - -1, 1: selected player's perspective\n",
    "        if perspective is None:\n",
    "            return self.raw_board\n",
    "        elif perspective == \"played\":\n",
    "            return [k * self.played for k in self.raw_board]\n",
    "        elif perspective == \"to_play\":\n",
    "            return [k * -self.played for k in self.raw_board]\n",
    "        return [k * perspective for k in self.raw_board]\n",
    "\n",
    "    def reward(self, perspective=None):\n",
    "        if perspective is None:\n",
    "            return self.raw_reward\n",
    "        elif perspective == \"played\":\n",
    "            return self.raw_reward * self.played\n",
    "        elif perspective == \"to_play\":\n",
    "            return self.raw_reward * -self.played\n",
    "        return self.raw_reward * perspective\n",
    "        \n",
    "    def _eval_win(self, player):\n",
    "        # check if player has won - enumerate all win positions\n",
    "        win_positions = np.array([\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],\n",
    "            [0, 4, 8], [2, 4, 6],\n",
    "        ])\n",
    "        has_win = np.any(\n",
    "            np.all((\n",
    "                np.array(self.raw_board)[win_positions.ravel()] == player\n",
    "            ).reshape(*win_positions.shape), axis=1)\n",
    "        )\n",
    "        return has_win\n",
    "    \n",
    "    def step(self, action, debug=False):\n",
    "        if self.raw_board[action] == 0:\n",
    "            self.raw_board[action] = -self.played\n",
    "        else:\n",
    "            # invalid action\n",
    "            if debug:\n",
    "                print(\"step: invalid action\")\n",
    "            if self.terminated is False:\n",
    "                # automatic win for other player\n",
    "                self.raw_reward = self.played\n",
    "            self.terminated = True\n",
    "\n",
    "        # player played\n",
    "        self.played = -self.played\n",
    "\n",
    "        if self._eval_win(self.played):\n",
    "            if debug:\n",
    "                print(\"step: eval_win\")\n",
    "            if self.terminated is False:\n",
    "                self.raw_reward = self.played\n",
    "            self.terminated = True\n",
    "\n",
    "        if len(self.valid_actions()) == 0:\n",
    "            if debug:\n",
    "                print(\"step: no more valid actions\")\n",
    "            # tie if not already terminated?\n",
    "            self.terminated = True\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def render(self):\n",
    "        for j in range(3):\n",
    "            print(\"[\", end=\" \")\n",
    "            for i in range(3):\n",
    "                val = self.raw_board[j * 3 + i]\n",
    "                s = \" \"\n",
    "                if val == 1:\n",
    "                    s = \"O\"\n",
    "                elif val == -1:\n",
    "                    s = \"X\"\n",
    "                print(s, end=\" \")\n",
    "            print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_win_positions_connect4():\n",
    "    win_positions = []\n",
    "    # vertical check\n",
    "    for col in range(7):\n",
    "        c = np.array([[0, 7, 14, 21], [7, 14, 21, 28], [14, 21, 28, 35]]) + col\n",
    "        c = c.tolist()\n",
    "        win_positions.extend(c)\n",
    "    # horizontal check\n",
    "    for row in range(6):\n",
    "        c = np.array([[0, 1, 2, 3], [1, 2, 3, 4], [2, 3, 4, 5], [3, 4, 5, 6]]) + 7*row\n",
    "        c = c.tolist()\n",
    "        win_positions.extend(c)\n",
    "    # diagonal (top-left to bottom-right check)\n",
    "    start_pts = [0, 1, 2, 3, 7, 8, 9, 10, 14, 15, 16, 17]\n",
    "    c = []\n",
    "    for pt in start_pts:\n",
    "        ci = [(pt + 8*i) for i in range(4)]\n",
    "        c.append(ci)\n",
    "    win_positions.extend(c)\n",
    "    # diagonal (top-right to bottom-left check)\n",
    "    start_pts = [3, 4, 5, 6, 10, 11, 12, 13, 17, 18, 19, 20]\n",
    "    c = []\n",
    "    for pt in start_pts:\n",
    "        ci = [(pt + 6*i) for i in range(4)]\n",
    "        c.append(ci)\n",
    "    win_positions.extend(c)\n",
    "    win_positions = np.array(win_positions)\n",
    "    return win_positions\n",
    "\n",
    "def valid_action_connect4(board):\n",
    "    valid_actions = []\n",
    "    for i in range(7):\n",
    "        row_idxs = list(reversed([(i + j * 7) for j in range(6)]))\n",
    "        for idx in row_idxs:\n",
    "            if board[idx] == 0:\n",
    "                valid_actions.append(idx)\n",
    "                break\n",
    "    return valid_actions\n",
    "\n",
    "class Connect4:\n",
    "    def __init__(self, board=None, played=None, terminated=None, reward=None):\n",
    "        # board and reward is not \"perspective corrected\"\n",
    "        # player - O: 1, X: -1\n",
    "\n",
    "        # self.raw_board:\n",
    "        # [  0  1  2  3  4  5  6\n",
    "        #    7  8  9 10 11 12 13\n",
    "        #   14 15 16 17 18 19 20\n",
    "        #   21 22 23 24 25 26 27\n",
    "        #   28 29 30 31 32 33 34\n",
    "        #   35 36 37 38 39 40 41 ]\n",
    "        if board is None:\n",
    "            self.raw_board = [0 for i in range(42)]\n",
    "        else:\n",
    "            self.raw_board = list(board)\n",
    "\n",
    "        # game starts with a change of player, so set to opponent\n",
    "        if played is None:\n",
    "            self.played = -1\n",
    "        else:\n",
    "            self.played = played\n",
    "\n",
    "        self.terminated = False if terminated is None else terminated\n",
    "\n",
    "        if reward is None:\n",
    "            self.raw_reward = 0\n",
    "        else:\n",
    "            self.raw_reward = reward\n",
    "\n",
    "    def state(self):\n",
    "        return {\n",
    "            \"board\": self.raw_board,\n",
    "            \"played\": self.played,\n",
    "            \"terminated\": self.terminated,\n",
    "            \"reward\": self.raw_reward\n",
    "        }\n",
    "\n",
    "    def valid_actions(self):\n",
    "        ret = []\n",
    "        if self.terminated:\n",
    "            return ret\n",
    "        ret = valid_action_connect4(self.raw_board)\n",
    "        return ret\n",
    "\n",
    "    def board(self, perspective=None):\n",
    "        # perspective\n",
    "        # - None: raw_board\n",
    "        # - played: \"played\" player's perspective\n",
    "        # - to_play: \"to_play\" player's perspective\n",
    "        # - -1, 1: selected player's perspective\n",
    "        if perspective is None:\n",
    "            return self.raw_board\n",
    "        elif perspective == \"played\":\n",
    "            return [k * self.played for k in self.raw_board]\n",
    "        elif perspective == \"to_play\":\n",
    "            return [k * -self.played for k in self.raw_board]\n",
    "        return [k * perspective for k in self.raw_board]\n",
    "\n",
    "    def reward(self, perspective=None):\n",
    "        if perspective is None:\n",
    "            return self.raw_reward\n",
    "        elif perspective == \"played\":\n",
    "            return self.raw_reward * self.played\n",
    "        elif perspective == \"to_play\":\n",
    "            return self.raw_reward * -self.played\n",
    "        return self.raw_reward * perspective\n",
    "        \n",
    "    def _eval_win(self, player):\n",
    "        # check if player has won - enumerate all win positions\n",
    "        win_positions = get_win_positions_connect4()\n",
    "        has_win = np.any(\n",
    "            np.all((\n",
    "                np.array(self.raw_board)[win_positions.ravel()] == player\n",
    "            ).reshape(*win_positions.shape), axis=1)\n",
    "        )\n",
    "        return has_win\n",
    "    \n",
    "    def step(self, action, debug=False):\n",
    "        if self.terminated is False and action in self.valid_actions():\n",
    "        #if self.raw_board[action] == 0:\n",
    "            self.raw_board[action] = -self.played\n",
    "        else:\n",
    "            # invalid action\n",
    "            if debug:\n",
    "                print(\"step: invalid action\")\n",
    "            if self.terminated is False:\n",
    "                # automatic win for other player\n",
    "                self.raw_reward = self.played\n",
    "            self.terminated = True\n",
    "\n",
    "        # player played\n",
    "        self.played = -self.played\n",
    "\n",
    "        if self._eval_win(self.played):\n",
    "            if debug:\n",
    "                print(\"step: eval_win\")\n",
    "            if self.terminated is False:\n",
    "                self.raw_reward = self.played\n",
    "            self.terminated = True\n",
    "\n",
    "        if len(self.valid_actions()) == 0:\n",
    "            if debug:\n",
    "                print(\"step: no more valid actions\")\n",
    "            # tie if not already terminated?\n",
    "            self.terminated = True\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def render(self):\n",
    "        for j in range(6):\n",
    "            print(\"[\", end=\" \")\n",
    "            for i in range(7):\n",
    "                val = self.raw_board[j * 7 + i]\n",
    "                s = \" \"\n",
    "                if val == 1:\n",
    "                    s = \"O\"\n",
    "                elif val == -1:\n",
    "                    s = \"X\"\n",
    "                print(s, end=\" \")\n",
    "            print(\"|\", end=\" \")\n",
    "            for i in range(7):\n",
    "                idx = j * 7 + i\n",
    "                print(\"%2d\" % idx, end=\" \")\n",
    "            print(\"]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_rollout(g, n_steps=None, debug=False):\n",
    "    current_player = g.played * -1\n",
    "    if debug:\n",
    "        print(\"current player:\", current_player)\n",
    "    step = 0\n",
    "    while g.terminated is False:\n",
    "        valid_actions = g.valid_actions()\n",
    "        action_idx = np.random.choice(len(valid_actions))\n",
    "        action = valid_actions[action_idx]\n",
    "        g.step(action)\n",
    "        if debug:\n",
    "            print(\"player\", \"%2d\" % g.played, \"plays  \")\n",
    "            g.render()\n",
    "        step += 1\n",
    "        if n_steps is not None:\n",
    "            if step >= n_steps:\n",
    "                break\n",
    "    if debug:\n",
    "        print(\"final board state\")\n",
    "        g.render()\n",
    "        print(\"final reward (as current player):\",\n",
    "              g.reward(perspective=current_player))\n",
    "    return g.reward(perspective=current_player)\n",
    "\n",
    "def value_nn_rollout(g, value_nn, debug=False):\n",
    "    current_player = g.played * -1\n",
    "    if debug:\n",
    "        print(\"current player:\", current_player)\n",
    "    if g.terminated is True:\n",
    "        return g.reward(perspective=current_player)\n",
    "    board = g.board(perspective=current_player)\n",
    "    with torch.no_grad():\n",
    "        reward = value_nn(torch.tensor([board], dtype=torch.float32))[0][0].cpu().numpy()\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, prior, game_state, game_class=None, node_id=None):\n",
    "        self.prior = prior\n",
    "        self.game_state = game_state\n",
    "        self.game_class = game_class\n",
    "        self.node_id = node_id\n",
    "\n",
    "        self.children = {}\n",
    "        self.visit_count = 0\n",
    "        self.value_sum = 0\n",
    "\n",
    "    def value(self):\n",
    "        return self.value_sum / self.visit_count\n",
    "\n",
    "    def is_leaf_node(self):\n",
    "        return len(self.children) == 0\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s: prior: %5.2f, n_child: %d, n_visit: %d, value_sum: %d\" %\\\n",
    "            (str(self.node_id), self.prior, len(self.children), self.visit_count, self.value_sum)\n",
    "    \n",
    "    def restore_game(self):\n",
    "        if self.game_class:\n",
    "            return self.game_class(**self.game_state)\n",
    "        return None\n",
    "\n",
    "    def print_tree(self, depth=None):\n",
    "        # currently max depth = 4\n",
    "        if depth is None:\n",
    "            depth = 999\n",
    "        print(self)\n",
    "        for i1 in self.children.keys():\n",
    "            n1 = self.children[i1]\n",
    "            print(\" \", n1)\n",
    "            if depth >= 2:\n",
    "                for i2 in n1.children.keys():\n",
    "                    n2 = n1.children[i2]\n",
    "                    print(\"   \", n2)\n",
    "                    if depth >= 3:\n",
    "                        for i3 in n2.children.keys():\n",
    "                            n3 = n2.children[i3]\n",
    "                            print(\"     \", n3)\n",
    "                            if depth >= 4:\n",
    "                                for i4 in n3.children.keys():\n",
    "                                    n4 = n3.children[i4]\n",
    "                                    print(\"       \", n4)\n",
    "\n",
    "def rollout(node, search_path, value_nn=None, debug=False):\n",
    "    if debug:\n",
    "        print(\"rollout from\", node.node_id)\n",
    "    g = node.restore_game()\n",
    "    if debug:\n",
    "        g.render()\n",
    "    current_player = g.played * -1\n",
    "    if value_nn is None:\n",
    "        reward = random_rollout(g, debug=False)\n",
    "    else:\n",
    "        reward = value_nn_rollout(g, value_nn, debug=False)\n",
    "    if debug:\n",
    "        print(\"playing as\", current_player, \"reward:\", reward)\n",
    "        g.render()\n",
    "\n",
    "    mod_reward = reward\n",
    "    for n in reversed(search_path):\n",
    "        n.value_sum += mod_reward\n",
    "        n.visit_count += 1\n",
    "        mod_reward *= -1\n",
    "\n",
    "def ucb1(parent, child, simple=False):\n",
    "    if simple is True:\n",
    "        if child.visit_count == 0:\n",
    "            return np.inf\n",
    "        return (-child.value_sum / child.visit_count) + 2 * np.sqrt(np.log(parent.visit_count) / child.visit_count)\n",
    "\n",
    "    # Definition from http://joshvarty.github.io/AlphaZero/\n",
    "    prior_score = child.prior * np.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
    "    if child.visit_count > 0:\n",
    "        # The value of the child is from the perspective of the opposing player\n",
    "        value_score = -child.value()\n",
    "    else:\n",
    "        value_score = 0\n",
    "\n",
    "    return value_score + prior_score\n",
    "\n",
    "def add_exploration_noise(node):\n",
    "    root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi\n",
    "    root_exploration_fraction = .25\n",
    "    actions = node.children.keys()\n",
    "    noise = np.random.gamma(root_dirichlet_alpha, 1, len(actions))\n",
    "    frac = root_exploration_fraction\n",
    "    for a, n in zip(actions, noise):\n",
    "        node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
    "\n",
    "def expand_node(node, policy_nn=None):\n",
    "    game_class = node.game_class\n",
    "    g = node.restore_game()\n",
    "    current_player = g.played * -1\n",
    "    current_board = g.board(perspective=current_player)\n",
    "    valid_actions = g.valid_actions()\n",
    "    if policy_nn is None:\n",
    "        probs = {}\n",
    "        for action in valid_actions:\n",
    "            prob = 1./len(valid_actions)\n",
    "            probs[action] = prob\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            probs = policy_nn(torch.tensor([current_board], dtype=torch.float32))[0].cpu().numpy()\n",
    "\n",
    "    for action in valid_actions:\n",
    "        g = node.restore_game().step(action)\n",
    "        node.children[action] = Node(\n",
    "            probs[action],\n",
    "            g.state(),\n",
    "            game_class,\n",
    "            node.node_id + \"-\" + str(action)\n",
    "        )\n",
    "    return valid_actions\n",
    "\n",
    "def run_mcts(node, policy_nn=None, value_nn=None, num_simulations=5, debug=False):\n",
    "    game_class = node.game_class\n",
    "    for i in range(num_simulations):\n",
    "        current = node\n",
    "        search_path = [current]\n",
    "        if debug:\n",
    "            print()\n",
    "            print(\"simulation:\", i)\n",
    "        while not current.is_leaf_node():\n",
    "            #print(\"not_leaf_node\")\n",
    "            ucb1_max_value = None\n",
    "            ucb1_action = None\n",
    "            for action, child_node in current.children.items():\n",
    "                if ucb1_max_value is None:\n",
    "                    ucb1_max_value = ucb1(current, child_node, simple=True if policy_nn is None else False)\n",
    "                    ucb1_action = action\n",
    "                else:\n",
    "                    ucb1_value = ucb1(current, child_node, simple=True if policy_nn is None else False)\n",
    "                    if ucb1_max_value < ucb1_value:\n",
    "                        ucb1_max_value = ucb1_value\n",
    "                        ucb1_action = action\n",
    "            #print(\"action: select\", ucb1_action, ucb1_max_value)\n",
    "            current = current.children[ucb1_action]\n",
    "            search_path.append(current)\n",
    "        #print([str(k) for k in search_path])\n",
    "        if debug:\n",
    "            print(\"initial:\")\n",
    "            node.print_tree()\n",
    "\n",
    "        # should be leaf node\n",
    "        #print(\"is_leaf_node\")\n",
    "        if current.visit_count == 0:\n",
    "            #print(\"first rollout\")\n",
    "            rollout(current, search_path, value_nn, debug=debug)\n",
    "            if debug:\n",
    "                node.print_tree()\n",
    "        else:\n",
    "            #print(\"expand and rollout\")\n",
    "            valid_actions = expand_node(current, policy_nn)\n",
    "\n",
    "            if len(valid_actions) == 0:\n",
    "                g = current.restore_game()\n",
    "                reward = g.reward(perspective=\"to_play\")\n",
    "                for n in reversed(search_path):\n",
    "                    n.value_sum += reward\n",
    "                    n.visit_count += 1\n",
    "                    reward *= -1\n",
    "            else:\n",
    "                first_action = valid_actions[0]\n",
    "                current = current.children[first_action]\n",
    "                search_path.append(current)\n",
    "\n",
    "                rollout(current, search_path, value_nn, debug=debug)\n",
    "            if debug:\n",
    "                node.print_tree()\n",
    "    return node\n",
    "\n",
    "def get_visit_counts(node, argmax=True):\n",
    "    db = []\n",
    "    g = node.restore_game()\n",
    "    visit_counts = [0 for i in range(len(g.raw_board))]\n",
    "    for n in node.children:\n",
    "        db.append([n, node.children[n].visit_count])\n",
    "        visit_counts[n] = node.children[n].visit_count\n",
    "    if len(db) == 0:\n",
    "        return None, visit_counts\n",
    "    #next_action = sorted(db, key=lambda x: x[1], reverse=True)[0][0]\n",
    "    if argmax is True:\n",
    "        next_action = np.argmax(visit_counts)\n",
    "    else:\n",
    "        next_action = np.random.choice(len(visit_counts), p=np.array(visit_counts) / np.sum(visit_counts))\n",
    "    return next_action, visit_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_class = Connect4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "print(g.valid_actions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for action in [38]:\n",
    "    g.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "valid_actions = expand_node(node)\n",
    "add_exploration_noise(node)\n",
    "run_mcts(node, num_simulations=400, debug=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_self_play(game_class, policy_nn, value_nn, num_simulations=100, run_status=True, explore_threshold=0, debug=False):\n",
    "    action_sequence = []\n",
    "    stats = []\n",
    "    step = 0\n",
    "    while True:\n",
    "        if run_status:\n",
    "            print(\".\", end=\"\")\n",
    "        g = game_class()\n",
    "        for action in action_sequence:\n",
    "            g.step(action)\n",
    "        node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "        valid_actions = expand_node(node, policy_nn)\n",
    "        add_exploration_noise(node)\n",
    "        run_mcts(node, policy_nn, value_nn, num_simulations=num_simulations, debug=debug)\n",
    "        if step < explore_threshold:\n",
    "            next_action, visit_counts = get_visit_counts(node, argmax=False)\n",
    "        else:\n",
    "            next_action, visit_counts = get_visit_counts(node, argmax=True)\n",
    "        if g.terminated is True or next_action is None:\n",
    "            for idx, (to_play, board, visit_prob, _) in enumerate(stats):\n",
    "                reward = g.reward(perspective=to_play)\n",
    "                stats[idx][-1] = reward\n",
    "            break\n",
    "        to_play = -g.played\n",
    "        board = g.board(perspective=to_play)\n",
    "        visit_prob = visit_counts / np.sum(visit_counts)\n",
    "        stats.append([to_play, board, visit_prob, 0])\n",
    "        action_sequence.append(next_action)\n",
    "        step += 1\n",
    "    if run_status:\n",
    "        print()\n",
    "    return action_sequence, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_game_status(g):\n",
    "    g.render()\n",
    "    print(\"%2d\" % g.played, \"b:     \", g.board(perspective=\"played\"), end=\"\")\n",
    "    print(\" | T:\", g.terminated, \"| R:\", g.reward(perspective=\"played\"), \"[\" + str(g.reward()) + \"]\")\n",
    "    print(\"%2d\" % -g.played, \"next_a:\", g.valid_actions())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "render_game_status(g)\n",
    "#for action in [3, 2, 0, 1]: # connect2\n",
    "#for action in [3, 2, 0, 6, 4, 5, 7, 8]: # tictactoe\n",
    "for action in [38, 37, 31, 30, 24, 23, 17]: # connect4\n",
    "    g.step(action)\n",
    "    render_game_status(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "random_rollout(g, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_size = len(game_class().board())\n",
    "action_size = len(game_class().board())\n",
    "value_nn = nn.Sequential(\n",
    "    nn.Linear(state_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1),\n",
    "    nn.Tanh()\n",
    ")\n",
    "policy_nn = nn.Sequential(\n",
    "    nn.Linear(state_size, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, action_size),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "value_nn.train()\n",
    "policy_nn.train()\n",
    "optimizer_value = optim.Adam(value_nn.parameters(), lr=.005)\n",
    "optimizer_policy = optim.Adam(policy_nn.parameters(), lr=.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "g.step(38).step(37).step(31).step(30).step(24).step(23)\n",
    "render_game_status(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def example_output(game_class, policy_nn, value_nn):\n",
    "    g = game_class()\n",
    "    valid_actions = g.valid_actions()\n",
    "    board = g.board(perspective=\"to_play\")\n",
    "    #print(np.array(board).reshape(6, 7))\n",
    "    pcy = policy_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()\n",
    "    pcy = np.array([(k if i in valid_actions else 0.) for i, k in enumerate(pcy)])\n",
    "    print(np.array(pcy.reshape(6, 7) * 1000, dtype=np.int))\n",
    "    print(np.argmax(pcy), value_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()[0])\n",
    "    g.step(38).step(37).step(31).step(30).step(24).step(23)\n",
    "    valid_actions = g.valid_actions()\n",
    "    board = g.board(perspective=\"to_play\")\n",
    "    #print(np.array(board).reshape(6, 7))\n",
    "    pcy = policy_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()\n",
    "    pcy = np.array([(k if i in valid_actions else 0.) for i, k in enumerate(pcy)])\n",
    "    print(np.array(pcy.reshape(6, 7) * 1000, dtype=np.int))\n",
    "    print(np.argmax(pcy), value_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()[0])\n",
    "example_output(game_class, policy_nn, value_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "g.step(38).step(37).step(31).step(30).step(24).step(23)\n",
    "board = g.board(perspective=\"to_play\")\n",
    "print(np.array(board).reshape(6, 7))\n",
    "pcy = policy_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()\n",
    "pcy = np.array([(k if i in valid_actions else 0.) for i, k in enumerate(pcy)])\n",
    "print(np.array(pcy.reshape(6, 7) * 1000, dtype=np.int))\n",
    "print(np.argmax(pcy), value_nn(torch.tensor([board], dtype=torch.float32))[0].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "g.step(38).step(37).step(31).step(30)\n",
    "render_game_status(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "action_sequence=[38,37,31,30,24,23]\n",
    "for action in action_sequence:\n",
    "    g.step(action)\n",
    "node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "valid_actions = expand_node(node, policy_nn)\n",
    "run_mcts(node, policy_nn, value_nn, num_simulations=25, debug=False)\n",
    "next_action_, visit_counts_ = get_visit_counts(node, argmax=True)\n",
    "print(visit_counts_, next_action_)\n",
    "#node.print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_output(game_class, policy_nn, value_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "action_sequence=[]\n",
    "for action in action_sequence:\n",
    "    g.step(action)\n",
    "node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "print(np.array(g.board(perspective=\"to_play\")).reshape(6, 7))\n",
    "valid_actions = expand_node(node)\n",
    "run_mcts(node, num_simulations=1000, debug=False)\n",
    "next_action_, visit_counts_ = get_visit_counts(node, argmax=True)\n",
    "print(\">\", next_action_, visit_counts_)\n",
    "node.print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "action_sequence=[38,37,31,30,24,23]\n",
    "for action in action_sequence:\n",
    "    g.step(action)\n",
    "node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "print(np.array(g.board(perspective=\"to_play\")).reshape(6, 7))\n",
    "valid_actions = expand_node(node)\n",
    "run_mcts(node, num_simulations=1000, debug=False)\n",
    "next_action_, visit_counts_ = get_visit_counts(node, argmax=True)\n",
    "print(\">\", next_action_, visit_counts_)\n",
    "node.print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#example_output(game_class, policy_nn, value_nn)\n",
    "\n",
    "g = game_class()\n",
    "#action_sequence=[38,37,31,30]#,24,23]\n",
    "action_sequence = [38,31,37,39,36,35,24,17,32,25,18]\n",
    "for action in action_sequence:\n",
    "    g.step(action)\n",
    "node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "print(np.array(g.board(perspective=\"to_play\")).reshape(6, 7))\n",
    "valid_actions = expand_node(node, policy_nn)\n",
    "run_mcts(node, policy_nn, value_nn, num_simulations=100, debug=False)\n",
    "next_action_, visit_counts_ = get_visit_counts(node, argmax=True)\n",
    "print(\">\", next_action_, visit_counts_)\n",
    "node.print_tree(depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(300):\n",
    "    print(\"[\", j, \"]\", end=\" \")\n",
    "    policy_nn.eval()\n",
    "    value_nn.eval()\n",
    "    \n",
    "    #\"\"\"\n",
    "    if j % 50 == 0:\n",
    "        example_output(game_class, policy_nn, value_nn)\n",
    "\n",
    "        g = game_class()\n",
    "        action_sequence=[38,31,37,39,36,35,24,17,32,25,18]#[38,37,31,30]\n",
    "        for action in action_sequence:\n",
    "            g.step(action)\n",
    "        node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "        valid_actions = expand_node(node, policy_nn)\n",
    "        run_mcts(node, policy_nn, value_nn, num_simulations=25, debug=False)\n",
    "        next_action_, visit_counts_ = get_visit_counts(node, argmax=True)\n",
    "        print(\">\", next_action_, visit_counts_)\n",
    "        node.print_tree(depth=1)\n",
    "    #\"\"\"\n",
    "    training_set = []\n",
    "    action_sequences = []\n",
    "    #for i in range(50):\n",
    "    batch_size = 512\n",
    "    while len(training_set) < batch_size:\n",
    "        print(\".\", end=\"\")\n",
    "        action_sequence, stats = run_self_play(game_class, policy_nn, value_nn, 100, run_status=False, explore_threshold=15)\n",
    "        action_sequences.append(action_sequence)\n",
    "        training_set.extend(stats)\n",
    "    if j % 50 == 0:\n",
    "        for action_sequence in action_sequences:\n",
    "            print(action_sequence)\n",
    "    #print()\n",
    "    training_set = training_set[:batch_size]\n",
    "    #training_set = get_examples(game_class, policy_nn, value_nn)\n",
    "    _, board, visit_prob, reward = [np.array(k) for k in list(zip(*training_set))]\n",
    "\n",
    "    #print(len(board))\n",
    "    policy_nn.train()\n",
    "    value_nn.train()\n",
    "\n",
    "    optimizer_policy.zero_grad()\n",
    "    optimizer_value.zero_grad()\n",
    "\n",
    "    loss_value = F.mse_loss(\n",
    "        torch.flatten(value_nn(torch.tensor(board, dtype=torch.float32))),\n",
    "        torch.tensor(reward, dtype=torch.float32)\n",
    "    )\n",
    "    loss_policy = torch.mean(-torch.sum(\n",
    "        torch.tensor(visit_prob, dtype=torch.float32) *\\\n",
    "        torch.log(policy_nn(torch.tensor(board, dtype=torch.float32))),\n",
    "    dim=1))\n",
    "    print(loss_policy.item(), loss_value.item())\n",
    "    loss_policy.backward()\n",
    "    loss_value.backward()\n",
    "\n",
    "    optimizer_policy.step()\n",
    "    optimizer_value.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = game_class()\n",
    "g.step(4).step(8).step(7)#.step(0).step(5).step(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.board(perspective=\"to_play\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array([\"%d\" % i for i in g.board(perspective=\"to_play\")]).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "value_nn(torch.tensor(g.board(perspective=\"to_play\"), dtype=torch.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = policy_nn(torch.tensor([g.board(perspective=\"to_play\")], dtype=torch.float32))\n",
    "np.array([\"%5.2f\" % i for i in res[0]]).reshape(3, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "            'model_state_dict': policy_nn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_policy.state_dict(),\n",
    "            }, \"policy_nn_0102_v2_clean.pth\")\n",
    "torch.save({\n",
    "            'model_state_dict': value_nn.state_dict(),\n",
    "            'optimizer_state_dict': optimizer_value.state_dict(),\n",
    "            }, \"value_nn_0102_v2_clean.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlphaZero Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sequence = []\n",
    "while True:\n",
    "    g = game_class()\n",
    "    for action in action_sequence:\n",
    "        g.step(action)\n",
    "    g.render()\n",
    "    print()\n",
    "    action = input()\n",
    "    action = int(action)\n",
    "    action_sequence.append(action)\n",
    "    \n",
    "    g = game_class()\n",
    "    for action in action_sequence:\n",
    "        g.step(action)\n",
    "    g.render()\n",
    "    print()\n",
    "    node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "    valid_actions = expand_node(node, policy_nn)\n",
    "    run_mcts(node, policy_nn, value_nn, num_simulations=100, debug=False)\n",
    "    next_action, _ = get_visit_counts(node)\n",
    "    if next_action is None:\n",
    "        break\n",
    "    action_sequence.append(next_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MCTS Interactive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_sequence = []\n",
    "while True:\n",
    "    g = game_class()\n",
    "    for action in action_sequence:\n",
    "        g.step(action)\n",
    "    g.render()\n",
    "    print()\n",
    "    action = input()\n",
    "    action = int(action)\n",
    "    action_sequence.append(action)\n",
    "    \n",
    "    g = game_class()\n",
    "    for action in action_sequence:\n",
    "        g.step(action)\n",
    "    g.render()\n",
    "    print()\n",
    "    node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "    valid_actions = expand_node(node)\n",
    "    run_mcts(node, num_simulations=100, debug=False)\n",
    "    next_action, _ = get_visit_counts(node)\n",
    "    if next_action is None:\n",
    "        break\n",
    "    action_sequence.append(next_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_tally = []\n",
    "\n",
    "for q in range(100):\n",
    "    action_sequence = []\n",
    "    print(q)\n",
    "    idx = 0\n",
    "    while True:\n",
    "        g = game_class()\n",
    "        for action in action_sequence:\n",
    "            g.step(action)\n",
    "        #g.render()\n",
    "        #print()\n",
    "        node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "        valid_actions = expand_node(node)\n",
    "        if idx < 5:\n",
    "            add_exploration_noise(node)\n",
    "        run_mcts(node, num_simulations=25, debug=False)\n",
    "        next_action, _ = get_visit_counts(node)\n",
    "        if next_action is None:\n",
    "            reward_tally.append(g.reward())\n",
    "            break\n",
    "        action_sequence.append(next_action)\n",
    "\n",
    "        g = game_class()\n",
    "        for action in action_sequence:\n",
    "            g.step(action)\n",
    "        #g.render()\n",
    "        #print()\n",
    "        node = Node(1, g.state(), game_class, node_id=\"root\")\n",
    "        valid_actions = expand_node(node, policy_nn)\n",
    "        if idx < 5:\n",
    "            add_exploration_noise(node)\n",
    "        run_mcts(node, policy_nn, value_nn, num_simulations=25, debug=False)\n",
    "        next_action, _ = get_visit_counts(node)\n",
    "        if next_action is None:\n",
    "            reward_tally.append(g.reward())\n",
    "            break\n",
    "        action_sequence.append(next_action)\n",
    "\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(np.array(reward_tally) == 1), np.sum(np.array(reward_tally) == -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
